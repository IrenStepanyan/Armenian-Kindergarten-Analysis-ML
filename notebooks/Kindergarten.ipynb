{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eea8ef87-0e23-4aa2-92af-180fccfa47ce",
   "metadata": {},
   "source": [
    "# ARMENIAN KINDERGARTEN MULTI-CITY ANALYSIS\n",
    "## Multi-City Data Scraping, Cleaning, and Comprehensive Statistical Analysis aross 10 Cities\n",
    "\n",
    "#### Student: Iren Stepanyan, Lusine Stepanyan\n",
    "#### Course: Algorithms and Programming Language (APL)\n",
    "#### Professor: V. Avetisyan  \n",
    "##### Date: Novermber 2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299899f2-7b9d-47d3-8ecb-7b377ab96592",
   "metadata": {},
   "source": [
    "## Project Objective:\n",
    "Collect and analyze kindergarten data from 10 Armenian cities to understand:\n",
    "1. Regional variations in demand and capacity\n",
    "2. Urban vs rural differences\n",
    "3. Patterns across different municipalities\n",
    "4. Statistical significance of inter-city differences\n",
    "\n",
    "## Data Sources:\n",
    "1. Yerevan: https://mankapartez.yerevan.am/order-view\n",
    "2. Ijevan: https://ijevan.infosys.am/Pages/KinderGarten/List.aspx\n",
    "3. Gyumri: https://gyumricity.am/Pages/KinderGarten/List.aspx\n",
    "4. Vanadzor: https://cmis.vanadzor.am/Pages/KinderGarten/List.aspx\n",
    "5. Armavir: https://armavircity.am/Pages/KinderGarten/List.aspx\n",
    "6. Kapan: https://kapan.am/pages/KinderGarten/List.aspx\n",
    "7. Abovyan: https://abovyan.am/Pages/KinderGarten/List.aspx\n",
    "8. Sevan: https://sevancity.am/Pages/KinderGarten/List.aspx\n",
    "9. Vaxarshapat: https://docs.ejmiatsin.am/Pages/KinderGarten/List.aspx\n",
    "10. Artashat: https://artashat.am/Pages/KinderGarten/List.aspx\n",
    "\n",
    "## Programming Launguage \n",
    "* `Python`\n",
    "\n",
    "## Libraries used: \n",
    "\n",
    "**Data Handling and Analysis:**\n",
    "\n",
    "* `pandas` – for data manipulation and analysis\n",
    "* `numpy` – for numerical operations\n",
    "\n",
    "**Visualization:**\n",
    "\n",
    "* `matplotlib.pyplot` – for plotting graphs\n",
    "* `seaborn` – for statistical data visualization\n",
    "\n",
    "**Statistics / Scientific Computing:**\n",
    "\n",
    "* `scipy.stats` – statistical tests (`chi2_contingency`, `f_oneway`, `ttest_ind`, `normaltest`, `kruskal`, `levene`, `shapiro`, `mannwhitneyu`)\n",
    "\n",
    "**Web Scraping / Automation:**\n",
    "\n",
    "* `selenium` – browser automation (`webdriver`, `By`, `WebDriverWait`, `expected_conditions`)\n",
    "* `bs4` (BeautifulSoup) – HTML parsing\n",
    "* `re` – regular expressions\n",
    "* `time` – for delays/waits during scraping\n",
    "\n",
    "**Utilities:**\n",
    "\n",
    "* `warnings` – for ignoring warnings\n",
    "\n",
    "**Visualization Style Settings:**\n",
    "\n",
    "* `plt.style.use('seaborn-v0_8-darkgrid')`\n",
    "* `sns.set_palette(\"husl\")`\n",
    "\n",
    "## Other tools: \n",
    "1. **Google colab:** for sharing and storing the code\n",
    "2. **CSV Files:** Serve as an intermediate format for saving and loading data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300beacc-abd0-4e13-86a7-bc86807e77fa",
   "metadata": {},
   "source": [
    "# Stage 0: Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5661bb57-7197-4076-8231-9737420d1d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import (chi2_contingency, f_oneway, ttest_ind, normaltest, \n",
    "                         kruskal, levene, shapiro, mannwhitneyu)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Web scraping\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "#print(\"All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fdc4d9-5ae3-4608-9848-6beeff3ad383",
   "metadata": {},
   "source": [
    "# Stage 1: Data Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7c6be5e-b529-474a-b74f-343ccef8689c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MULTI-CITY DATA SCRAPING\n",
      "======================================================================\n",
      "Total kindergartens scraped: 328\n",
      "Cities successfully scraped: 10\n",
      "\n",
      "Raw data shape: (328, 11)\n",
      "Columns: ['city', 'name', 'district', 'address', 'order_count', 'population', 'region', 'groups_count', 'registered_count', 'phone', 'capacity']\n",
      "\n",
      "Cities in dataset:\n",
      "city\n",
      "Yerevan        162\n",
      "Artashat        32\n",
      "Gyumri          24\n",
      "Vanadzor        23\n",
      "Kapan           18\n",
      "Armavir         17\n",
      "Ijevan          16\n",
      "Sevan           14\n",
      "Abovyan         12\n",
      "Vaxarshapat     10\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define Armenian cities and their kindergarten URLs\n",
    "ARMENIAN_CITIES = {\n",
    "    'Yerevan': {\n",
    "        'url': 'https://mankapartez.yerevan.am/order-view',\n",
    "        'type': 'angular',  # Dynamic Angular site\n",
    "        'population': 1093000,\n",
    "        'region': 'Capital'\n",
    "    },\n",
    "    'Ijevan': {\n",
    "        'url': 'https://ijevancity.am/Pages/KinderGarten/List.aspx',\n",
    "        'type': 'aspnet',  # ASP.NET site\n",
    "        'population': 21000,\n",
    "        'region': 'Tavush'\n",
    "    },\n",
    "    'Gyumri': {\n",
    "        'url': 'https://gyumricity.am/Pages/KinderGarten/List.aspx',\n",
    "        'type': 'aspnet',\n",
    "        'population': 121000,\n",
    "        'region': 'Shirak'\n",
    "    },\n",
    "    'Vanadzor': {\n",
    "        'url': 'https://cmis.vanadzor.am/Pages/KinderGarten/List.aspx',\n",
    "        'type': 'aspnet',\n",
    "        'population': 82000,\n",
    "        'region': 'Lori'\n",
    "    },\n",
    "    'Armavir': {\n",
    "        'url': 'https://armavircity.am/Pages/KinderGarten/List.aspx',\n",
    "        'type': 'aspnet',\n",
    "        'population': 29000,\n",
    "        'region': 'Armavir'\n",
    "    },\n",
    "    'Kapan': {\n",
    "        'url': 'https://kapan.am/pages/KinderGarten/List.aspx',\n",
    "        'type': 'aspnet',\n",
    "        'population': 43000,\n",
    "        'region': 'Syunik'\n",
    "    },\n",
    "    'Abovyan': {\n",
    "        'url': 'https://abovyan.am/Pages/KinderGarten/List.aspx',\n",
    "        'type': 'aspnet',\n",
    "        'population': 46000,\n",
    "        'region': 'Kotayk'\n",
    "    },\n",
    "    'Sevan': {\n",
    "        'url': 'https://sevancity.am/Pages/KinderGarten/List.aspx',\n",
    "        'type': 'aspnet',\n",
    "        'population': 19000,\n",
    "        'region': 'Gegharkunik'\n",
    "    },\n",
    "    'Vaxarshapat': {\n",
    "        'url': 'https://docs.ejmiatsin.am/Pages/KinderGarten/List.aspx',\n",
    "        'type': 'aspnet',\n",
    "        'population': 47446,\n",
    "        'region': 'Armavir'\n",
    "    },\n",
    "     'Artashat': {\n",
    "        'url': 'https://artashat.am/Pages/KinderGarten/List.aspx',\n",
    "        'type': 'aspnet',\n",
    "        'population': 29040,\n",
    "        'region': 'Ararat'\n",
    "    }\n",
    "}\n",
    "\n",
    "def scrape_yerevan_style(driver, url, city_name):\n",
    "    \"\"\"Scrape Angular-based site (Yerevan style)\"\"\"\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    kindergartens = []\n",
    "    kinder_elements = driver.find_elements(By.CSS_SELECTOR, '.kinder-content')\n",
    "    \n",
    "    for element in kinder_elements:\n",
    "        try:\n",
    "            kg = {'city': city_name}\n",
    "            kg['name'] = element.find_element(By.CSS_SELECTOR, '.kinder-title a').text.strip()\n",
    "            \n",
    "            list_items = element.find_elements(By.CSS_SELECTOR, '.kinder-body ul li')\n",
    "            for item in list_items:\n",
    "                text = item.text.strip()\n",
    "                if 'Հերթագրված է' in text:\n",
    "                    match = re.search(r'(\\d+)\\s*երեխա', text)\n",
    "                    if match: kg['order_count'] = int(match.group(1))\n",
    "                elif 'Գործում է' in text and 'խումբ' in text:\n",
    "                    match = re.search(r'(\\d+)\\s*խումբ', text)\n",
    "                    if match: kg['groups_count'] = int(match.group(1))\n",
    "                elif 'Հաշվառված է' in text:\n",
    "                    match = re.search(r'(\\d+)\\s*երեխա', text)\n",
    "                    if match: kg['registered_count'] = int(match.group(1))\n",
    "                else:\n",
    "                    if 'address' not in kg and len(text) > 10:\n",
    "                        kg['address'] = text\n",
    "                    elif 'district' not in kg:\n",
    "                        kg['district'] = text\n",
    "            \n",
    "            if kg.get('name'):\n",
    "                kindergartens.append(kg)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    return kindergartens\n",
    "def scrape_aspnet_style(driver, url, city_name):\n",
    "    \"\"\"Scrape ASP.NET-based sites (Ijevan, Gyumri, etc.)\"\"\"\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    kindergartens = []\n",
    "    \n",
    "    # Get page source and parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Find all kindergarten items\n",
    "    items = soup.find_all('a', id=lambda x: x and 'rptGardenList' in str(x) and 'LbCategory' in str(x))\n",
    "    \n",
    "    for item in items:\n",
    "        try:\n",
    "            kg = {'city': city_name}\n",
    "            \n",
    "            # Extract name\n",
    "            name_div = item.find('div', class_='CategoryName')\n",
    "            if name_div:\n",
    "                kg['name'] = name_div.get_text(strip=True)\n",
    "            \n",
    "            # Extract from table rows\n",
    "            rows = item.find_all('tr')\n",
    "            for row in rows:\n",
    "                text = row.get_text(strip=True)\n",
    "                \n",
    "                # Extract address (contains map icon)\n",
    "                if 'glyphicon-map-marker' in str(row):\n",
    "                    kg['address'] = text.replace('glyphicon', '').strip()\n",
    "                \n",
    "                # Extract phone\n",
    "                elif 'glyphicon-earphone' in str(row):\n",
    "                    kg['phone'] = re.search(r'[\\d\\s\\-+()]+', text)\n",
    "                    if kg['phone']:\n",
    "                        kg['phone'] = kg['phone'].group(0).strip()\n",
    "                \n",
    "                # Extract groups (Գործում է X խումբ)\n",
    "                elif 'Գործում է' in text and 'խումբ' in text:\n",
    "                    match = re.search(r'(\\d+)\\s*խումբ', text)\n",
    "                    if match:\n",
    "                        kg['groups_count'] = int(match.group(1))\n",
    "                \n",
    "                # Extract capacity (Ցուցակային թիվ)\n",
    "                elif 'Ցուցակային' in text or 'թիվ' in text:\n",
    "                    match = re.search(r'(\\d+)', text)\n",
    "                    if match:\n",
    "                        kg['capacity'] = int(match.group(1))\n",
    "                \n",
    "                # Extract registered (Հաշվառված է X երեխա / Հաճախում է)\n",
    "                elif 'Հաշվառված' in text or 'Հաճախում' in text:\n",
    "                    match = re.search(r'(\\d+)\\s*երեխա', text)\n",
    "                    if match:\n",
    "                        kg['registered_count'] = int(match.group(1))\n",
    "                \n",
    "                # Extract waiting list (Հերթագրված է X երեխա)\n",
    "                elif 'Հերթագրված' in text:\n",
    "                    match = re.search(r'(\\d+)\\s*երեխա', text)\n",
    "                    if match:\n",
    "                        kg['order_count'] = int(match.group(1))\n",
    "            \n",
    "            if kg.get('name'):\n",
    "                kindergartens.append(kg)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing item: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return kindergartens\n",
    "\n",
    "def scrape_all_cities():\n",
    "    \"\"\"Scrape kindergarten data from all Armenian cities\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MULTI-CITY DATA SCRAPING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    \n",
    "    try:\n",
    "        for city_name, city_info in ARMENIAN_CITIES.items():\n",
    "            try:\n",
    "                if city_info['type'] == 'angular':\n",
    "                    city_data = scrape_yerevan_style(driver, city_info['url'], city_name)\n",
    "                else:\n",
    "                    city_data = scrape_aspnet_style(driver, city_info['url'], city_name)\n",
    "                \n",
    "                # Add city metadata\n",
    "                for kg in city_data:\n",
    "                    kg['population'] = city_info['population']\n",
    "                    kg['region'] = city_info['region']\n",
    "                \n",
    "                all_data.extend(city_data)                \n",
    "                time.sleep(2)  # Be polite to servers\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\" Error scraping {city_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Total kindergartens scraped: {len(all_data)}\")\n",
    "        print(f\"Cities successfully scraped: {len(set([kg['city'] for kg in all_data]))}\")\n",
    "        \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "# Execute multi-city scraping\n",
    "all_kindergartens = scrape_all_cities()\n",
    "df_raw = pd.DataFrame(all_kindergartens)\n",
    "\n",
    "print(f\"\\nRaw data shape: {df_raw.shape}\")\n",
    "print(f\"Columns: {df_raw.columns.tolist()}\")\n",
    "print(f\"\\nCities in dataset:\")\n",
    "print(df_raw['city'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97bc3a7-338a-44df-bfac-723f7b57c063",
   "metadata": {},
   "source": [
    "# Stage 2: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5662c0c-f052-482d-94bc-ed48d9695824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATA CLEANING\n",
      "======================================================================\n",
      "\n",
      "Initial data shape: (328, 11)\n",
      "Initial missing values:\n",
      " city                  0\n",
      "name                  0\n",
      "district            166\n",
      "address               0\n",
      "order_count           0\n",
      "population            0\n",
      "region                0\n",
      "groups_count         15\n",
      "registered_count     15\n",
      "phone               200\n",
      "capacity            162\n",
      "dtype: int64\n",
      "→ Removed 0 duplicate rows\n",
      "→ Removed 0 rows with missing critical data\n",
      "\n",
      "Cleaned data shape: (328, 12)\n",
      "Final missing values:\n",
      " city                  0\n",
      "name                  0\n",
      "district            166\n",
      "address               0\n",
      "order_count           0\n",
      "population            0\n",
      "region                0\n",
      "groups_count          0\n",
      "registered_count      0\n",
      "phone                 0\n",
      "capacity              0\n",
      "city_size             0\n",
      "dtype: int64\n",
      "\n",
      "Kindergartens per city:\n",
      "city\n",
      "Yerevan        162\n",
      "Artashat        32\n",
      "Gyumri          24\n",
      "Vanadzor        23\n",
      "Kapan           18\n",
      "Armavir         17\n",
      "Ijevan          16\n",
      "Sevan           14\n",
      "Abovyan         12\n",
      "Vaxarshapat     10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Cleaned data saved to: multi_city_kindergartens_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA CLEANING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def clean_multi_city_data(df):\n",
    "    \"\"\"Comprehensive cleaning for multi-city dataset\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    print(\"\\nInitial data shape:\", df_clean.shape)\n",
    "    print(\"Initial missing values:\\n\", df_clean.isnull().sum())\n",
    "    \n",
    "    # 1. Clean text columns\n",
    "    text_cols = ['name', 'address', 'phone']\n",
    "    for col in text_cols:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = df_clean[col].astype(str).str.strip()\n",
    "            df_clean[col] = df_clean[col].str.replace(r'\\s+', ' ', regex=True)\n",
    "    \n",
    "    # 2. Handle numeric columns\n",
    "    numeric_cols = ['order_count', 'registered_count', 'groups_count', 'capacity']\n",
    "    for col in numeric_cols:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce').fillna(0).astype(int)\n",
    "    \n",
    "    # 3. Remove duplicates\n",
    "    before_dup = len(df_clean)\n",
    "    df_clean = df_clean.drop_duplicates(subset=['city', 'name'], keep='first')\n",
    "    print(f\"→ Removed {before_dup - len(df_clean)} duplicate rows\")\n",
    "    \n",
    "    # 4. Remove rows with missing critical data\n",
    "    before_null = len(df_clean)\n",
    "    df_clean = df_clean.dropna(subset=['name', 'city'], how='any')\n",
    "    print(f\"→ Removed {before_null - len(df_clean)} rows with missing critical data\")\n",
    "    \n",
    "    # 5. Standardize city names and regions\n",
    "    df_clean['city'] = df_clean['city'].str.strip()\n",
    "    df_clean['region'] = df_clean['region'].str.strip()\n",
    "    \n",
    "    # 6. Add city size category\n",
    "    def categorize_city_size(pop):\n",
    "        if pop > 500000:\n",
    "            return 'Major City'\n",
    "        elif pop > 100000:\n",
    "            return 'Large City'\n",
    "        elif pop > 50000:\n",
    "            return 'Medium City'\n",
    "        elif pop > 20000:\n",
    "            return 'Small City'\n",
    "        else:\n",
    "            return 'Town'\n",
    "    \n",
    "    df_clean['city_size'] = df_clean['population'].apply(categorize_city_size)\n",
    "    \n",
    "    df_clean = df_clean.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nCleaned data shape: {df_clean.shape}\")\n",
    "    print(\"Final missing values:\\n\", df_clean.isnull().sum())\n",
    "    print(\"\\nKindergartens per city:\")\n",
    "    print(df_clean['city'].value_counts())\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "df_clean = clean_multi_city_data(df_raw)\n",
    "\n",
    "# Save cleaned data\n",
    "df_clean.to_csv('multi_city_kindergartens_cleaned.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"\\nCleaned data saved to: multi_city_kindergartens_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e32f65-5359-4168-b651-241771e12543",
   "metadata": {},
   "source": [
    "# Stage 3: Data Preprocessing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
